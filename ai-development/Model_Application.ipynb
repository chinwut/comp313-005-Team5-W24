{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09113b29",
   "metadata": {},
   "source": [
    "# Prepare for your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c9c990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0 has 19 frames; First frame shape: (21, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Courses\\2024Winter\\COMP313\\Code\\output_landmarks.csv\", header=None, names=[\"index\", \"landmark\", \"target\"])\n",
    "\n",
    "\n",
    "def convert_to_json(string):\n",
    "    try:\n",
    "        string = string.replace(\"'\", \"\\\"\")\n",
    "        string = string.replace(\"\\\"\\\"\\\"\", \"\\\"'\\\"\")\n",
    "        return json.loads(string)\n",
    "    except ValueError:\n",
    "        print(string)\n",
    "        return None\n",
    "\n",
    "\n",
    "df['landmark'] = df['landmark'].apply(json.loads)\n",
    "\n",
    "\n",
    "input_landmarks =df['landmark'].iloc[0]\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    # Access the ith element in 'landmarks'\n",
    "    landmarks_seq = df['landmark'].iloc[i]\n",
    "    # Calculate the number of frames\n",
    "    num_frames = len(landmarks_seq)\n",
    "    # For lists, manually determine the shape of the first frame (if available)\n",
    "    first_frame_shape = (len(landmarks_seq[0]), len(landmarks_seq[0][0])) if num_frames > 0 and len(landmarks_seq[0]) > 0 else \"No frames\"\n",
    "    print(f\"Sequence {i} has {num_frames} frames; First frame shape: {first_frame_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(input_landmarks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35fa1b",
   "metadata": {},
   "source": [
    "# Step 1: Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb9c4334-91ac-417a-a5b1-b77745003c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = load_model('model_sign_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c10e78",
   "metadata": {},
   "source": [
    "# Step 2: Prepare Your New Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81815526",
   "metadata": {},
   "source": [
    "# Step 3: Make Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e07cb3-386a-47a4-ba38-b43100502a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa2e69-748f-42c0-aea4-3733505595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a1c6a-72ab-4bac-9ded-5c929c65ff9f",
   "metadata": {},
   "source": [
    "# application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "faf433be-cbc5-4bea-8c11-4837b4fddfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def normalize_landmarks(landmarks_seq):\n",
    "    # This will hold the normalized sequences\n",
    "    normalized_seq = []\n",
    "    \n",
    "    for matrix in landmarks_seq:\n",
    "        # Ensure the matrix is not empty to avoid errors during scaling\n",
    "        if matrix:  # Checks if the matrix is not empty\n",
    "            matrix_np = np.array(matrix)\n",
    "            # Normalize the [21x3] matrix\n",
    "            # Note: scaler.fit_transform expects a 2D array, [samples, features]\n",
    "            # Reshape is used to ensure the matrix is 2D and to revert it back to original shape after scaling\n",
    "            scaled_matrix = scaler.fit_transform(matrix_np.reshape(-1, 3)).reshape(21, 3)\n",
    "            normalized_seq.append(scaled_matrix)\n",
    "        else:\n",
    "            # Handle empty matrices by appending an array of zeros or another placeholder\n",
    "            # This keeps the temporal structure consistent\n",
    "            normalized_seq.append(np.zeros((21, 3)))\n",
    "    \n",
    "    return normalized_seq\n",
    "\n",
    "input_landmarks =df['landmark'].iloc[0]\n",
    "normaized_landmarks =normalize_landmarks(input_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50b085db-b561-4dc6-b339-0ef9fb28727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pad the scaled landmark data\n",
    "max_frames = 125  # Target number of frames for each sequence\n",
    "\n",
    "def pad_sequence(sequence):\n",
    "    # Determine the number of frames to pad\n",
    "    num_frames_to_pad = max_frames - len(sequence)\n",
    "    \n",
    "    # Create frames of zeros to be appended\n",
    "    # Each frame is a (21, 3) array of zeros\n",
    "    padding_frames = [np.zeros((21, 3)) for _ in range(num_frames_to_pad)]\n",
    "    \n",
    "    # Append the padding frames to the sequence\n",
    "    padded_sequence = sequence + padding_frames\n",
    "    \n",
    "    return padded_sequence\n",
    "\n",
    "\n",
    "processed_landmarks =pad_sequence(normaized_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8782aea8-e726-4e7a-9e09-70223c128ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(processed_landmarks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2712efa3-b907-4dde-a883-c1138d48ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Sequence 1: [4 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_sequence(model, X_test, sequence_length=17, num_decoder_tokens=27, start_token_index=0):\n",
    "    \"\"\"\n",
    "    Predict sequences without relying on y_test, using a start token and iterative predictions.\n",
    "    \"\"\"\n",
    "    decoder_input_test = np.zeros((len(X_test), sequence_length, num_decoder_tokens))\n",
    "    decoder_input_test[:, 0, start_token_index] = 1  # Start with the start token\n",
    "    \n",
    "    for i in range(1, sequence_length):\n",
    "        predicted_sequences = model.predict([X_test, decoder_input_test])\n",
    "        next_token = np.argmax(predicted_sequences[:, i-1, :], axis=-1)\n",
    "        decoder_input_test = np.zeros_like(decoder_input_test)\n",
    "        for j in range(i + 1):\n",
    "            decoder_input_test[np.arange(len(X_test)), j, next_token if j == i else start_token_index] = 1\n",
    "\n",
    "    return predicted_sequences\n",
    "\n",
    "def trim_and_analyze_predicted_sequences(predicted_sequences, pad_token=0):\n",
    "    \"\"\"\n",
    "    Trim the predicted sequences by removing trailing pad tokens and analyze them.\n",
    "    \"\"\"\n",
    "    predicted_labels = np.argmax(predicted_sequences, axis=-1)  # Convert to label indices\n",
    "    \n",
    "    def trim_sequences(sequences, pad_token):\n",
    "        trimmed_sequences = []\n",
    "        for seq in sequences:\n",
    "            last_valid_index = -1\n",
    "            for idx, token in enumerate(seq):\n",
    "                if token != pad_token:\n",
    "                    last_valid_index = idx\n",
    "            trimmed_seq = seq[:last_valid_index+1] if last_valid_index != -1 else []\n",
    "            trimmed_sequences.append(trimmed_seq)\n",
    "        return trimmed_sequences\n",
    "    \n",
    "    predicted_labels_trimmed = trim_sequences(predicted_labels, pad_token)\n",
    "    \n",
    "    return predicted_labels_trimmed\n",
    "\n",
    "# Assuming `model` and `X_test` are already defined and prepared\n",
    "predicted_sequences = predict_sequence(model, X_test)\n",
    "predicted_labels_trimmed = trim_and_analyze_predicted_sequences(predicted_sequences)\n",
    "\n",
    "num_sequences_to_display = 10\n",
    "for i, seq in enumerate(predicted_labels_trimmed[:num_sequences_to_display]):\n",
    "    print(f\"Sequence {i+1}: {seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "383e69ee-1135-40cf-833f-639ba54b8ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Sequence 1: d c\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the label encoder from a file\n",
    "label_encoder = joblib.load('label_encoder.joblib')\n",
    "\n",
    "# Function to decode a single sequence of label indices into labels\n",
    "def decode_sequence(sequence, label_encoder, pad_token=None):\n",
    "    if pad_token is not None:\n",
    "        # Filter out padding tokens if your pad_token is not a valid label index\n",
    "        valid_indices = [index for index in sequence if index != pad_token]\n",
    "    else:\n",
    "        valid_indices = sequence\n",
    "    # Decode the sequence of indices to labels\n",
    "    labels = label_encoder.inverse_transform(valid_indices)\n",
    "    return labels\n",
    "\n",
    "# Decode all trimmed predicted sequences\n",
    "decoded_sequences = [decode_sequence(seq, label_encoder) for seq in predicted_labels_trimmed]\n",
    "\n",
    "# Display the decoded sequences\n",
    "for i, decoded_seq in enumerate(decoded_sequences[:num_sequences_to_display], 1):\n",
    "    print(f\"Decoded Sequence {i}: {' '.join(decoded_seq)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106483f-62a4-421c-aea3-50cd234b5450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
